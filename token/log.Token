ESTIMATE of time to complete assignment: 20 hours

	   Start  Time
     Date  Time   Spent Work completed
     ----  -----  ----  --------------
     10/9  20:00   4:00  Played with token implementation, planned program
     10/14 10:00   8:00  Coded up program
     10/15 12:00   6:00  Debugged
     10/16 17:00   5:00  Debugged, found more edge cases, documented
                  -----
		  23:00 TOTAL time spent


I discussed this project with Alex Ringlein, Tyler Dohrn, and Alan Liu.

The hardest part of this project was finding niche edge cases. It seems like most people
were capable of finishing the basic skeleton of the program in one sitting (with or without
some planning). But, finding - and correcting - edge cases was harder than expected.

Two things that REALLY helped were:
    1) Writing an "edge case" generator
    2) Learning pdb (python debugger)

Regarding the edge case generator, I wrote a simple python script that would generate
HUGE files filled with meta characters, escape characters, various whitespace combos,
quotations, and alphanumeric text. It was immensely helpful, but I'm afraid the extra
error protection I added to my code is making it run too slowly. On input files at
~1 mil chars, my program took about 10 times as long as yours. Hopefully my newfound
mastery of pdb will help me in my quest to get better with gdb.

My only question arising from this assignment is: how can I look for runtime bottlenecks
in my code? I'm assuming that mine runs in linear time, but just much slower.
I'm familiar with gprof, but I wanted to know if you have some sort of "checklist"
that might be helpful for finding slow functions.

I'm also interested in getting to know some of the more complicated "pythonic" 
language constructs that python-lovers are so obssessed with. I found myself
struggling to effectively use generators, iterators, and the more complex
loop constructs, since they don't allow flexibility in changing the index of the
current iteration without some tinkering.

